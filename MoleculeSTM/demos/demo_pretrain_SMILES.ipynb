{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for MoleculeSTM pretraining\n",
    "\n",
    "All the scripts can be found in `MoleculeSTM/pretrain.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Customize Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments\t Namespace(CL_neg_samples=1, SSL_emb_dim=256, SSL_loss='EBM_NCE', T=0.1, batch_size=4, dataset='PubChemSTM1K', dataspace_path='../data', decay=0, device=0, epochs=100, max_seq_len=512, megamolbart_input_dir='../data/pretrained_MegaMolBART/checkpoints', mol_lr=0.0001, mol_lr_scale=0.1, molecule_type='SMILES', normalize=True, num_workers=8, output_model_dir=None, seed=42, text_lr=0.0001, text_lr_scale=0.1, text_type='SciBERT', verbose=1)\n"
     ]
    }
   ],
   "source": [
    "# Set-up the environment variable to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--device\", type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"PubChemSTM1K\")\n",
    "parser.add_argument(\"--text_type\", type=str, default=\"SciBERT\", choices=[\"SciBERT\"])\n",
    "parser.add_argument(\"--molecule_type\", type=str, default=\"SMILES\", choices=[\"SMILES\", \"Graph\"])\n",
    "\n",
    "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "parser.add_argument(\"--text_lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--mol_lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--text_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--mol_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "parser.add_argument(\"--decay\", type=float, default=0)\n",
    "parser.add_argument(\"--verbose\", type=int, default=1)\n",
    "parser.add_argument(\"--output_model_dir\", type=str, default=None)\n",
    "\n",
    "########## for SciBERT ##########\n",
    "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
    "\n",
    "########## for MegaMolBART ##########\n",
    "parser.add_argument(\"--megamolbart_input_dir\", type=str, default=\"../data/pretrained_MegaMolBART/checkpoints\")\n",
    "\n",
    "########## for contrastive SSL ##########\n",
    "parser.add_argument(\"--SSL_loss\", type=str, default=\"EBM_NCE\", choices=[\"EBM_NCE\", \"InfoNCE\"])\n",
    "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
    "parser.add_argument(\"--CL_neg_samples\", type=int, default=1)\n",
    "parser.add_argument(\"--T\", type=float, default=0.1)\n",
    "parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
    "parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
    "parser.set_defaults(normalize=True)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "print(\"arguments\\t\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-30 12:36:54,712] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader as torch_DataLoader\n",
    "\n",
    "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from MoleculeSTM.datasets import (\n",
    "    PubChemSTM_Datasets_SMILES, PubChemSTM_SubDatasets_SMILES,\n",
    "    PubChemSTM_Datasets_Graph, PubChemSTM_SubDatasets_Graph,\n",
    "    PubChemSTM_Datasets_Raw_SMILES, PubChemSTM_SubDatasets_Raw_SMILES,\n",
    "    PubChemSTM_Datasets_Raw_Graph, PubChemSTM_SubDatasets_Raw_Graph\n",
    ")\n",
    "from MoleculeSTM.models import GNN, GNN_graphpred\n",
    "from MoleculeSTM.utils import prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network\n",
    "from MoleculeSTM.models.mega_molbart.mega_mol_bart import MegaMolBART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_index(num, shift):\n",
    "    arr = torch.arange(num) + shift\n",
    "    arr[-shift:] = torch.arange(shift)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def do_CL(X, Y, args):\n",
    "    if args.normalize:\n",
    "        X = F.normalize(X, dim=-1)\n",
    "        Y = F.normalize(Y, dim=-1)\n",
    "\n",
    "    if args.SSL_loss == 'EBM_NCE':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        neg_Y = torch.cat([Y[cycle_index(len(Y), i + 1)] for i in range(args.CL_neg_samples)], dim=0)\n",
    "        neg_X = X.repeat((args.CL_neg_samples, 1))\n",
    "\n",
    "        pred_pos = torch.sum(X * Y, dim=1) / args.T\n",
    "        pred_neg = torch.sum(neg_X * neg_Y, dim=1) / args.T\n",
    "\n",
    "        loss_pos = criterion(pred_pos, torch.ones(len(pred_pos)).to(pred_pos.device))\n",
    "        loss_neg = criterion(pred_neg, torch.zeros(len(pred_neg)).to(pred_neg.device))\n",
    "        CL_loss = (loss_pos + args.CL_neg_samples * loss_neg) / (1 + args.CL_neg_samples)\n",
    "\n",
    "        CL_acc = (torch.sum(pred_pos > 0).float() + torch.sum(pred_neg < 0).float()) / \\\n",
    "                 (len(pred_pos) + len(pred_neg))\n",
    "        CL_acc = CL_acc.detach().cpu().item()\n",
    "\n",
    "    elif args.SSL_loss == 'InfoNCE':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        B = X.size()[0]\n",
    "        logits = torch.mm(X, Y.transpose(1, 0))  # B*B\n",
    "        logits = torch.div(logits, args.T)\n",
    "        labels = torch.arange(B).long().to(logits.device)  # B*1\n",
    "\n",
    "        CL_loss = criterion(logits, labels)\n",
    "        pred = logits.argmax(dim=1, keepdim=False)\n",
    "        CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
    "\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "    return CL_loss, CL_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    epoch,\n",
    "    dataloader,\n",
    "    text_model, text_tokenizer,\n",
    "    molecule_model, MegaMolBART_wrapper=None):\n",
    "\n",
    "    text_model.train()\n",
    "    molecule_model.train()\n",
    "    text2latent.train()\n",
    "    mol2latent.train()\n",
    "\n",
    "    if args.verbose:\n",
    "        L = tqdm(dataloader)\n",
    "    else:\n",
    "        L = dataloader\n",
    "    \n",
    "    start_time = time.time()\n",
    "    accum_loss, accum_acc = 0, 0\n",
    "    for step, batch in enumerate(L):\n",
    "        description = batch[0]\n",
    "        molecule_data = batch[1]\n",
    "\n",
    "        description_tokens_ids, description_masks = prepare_text_tokens(\n",
    "            device=device, description=description, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
    "        description_output = text_model(input_ids=description_tokens_ids, attention_mask=description_masks)\n",
    "        description_repr = description_output[\"pooler_output\"]\n",
    "        description_repr = text2latent(description_repr)\n",
    "\n",
    "        molecule_data = list(molecule_data) # for SMILES_list\n",
    "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
    "            molecule_data, mol2latent=mol2latent,\n",
    "            molecule_type=molecule_type, MegaMolBART_wrapper=MegaMolBART_wrapper)\n",
    "\n",
    "        loss_01, acc_01 = do_CL(description_repr, molecule_repr, args)\n",
    "        loss_02, acc_02 = do_CL(molecule_repr, description_repr, args)\n",
    "        loss = (loss_01 + loss_02) / 2\n",
    "        acc = (acc_01 + acc_02) / 2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        accum_loss += loss.item()\n",
    "        accum_acc += acc\n",
    "    \n",
    "    accum_loss /= len(L)\n",
    "    accum_acc /= len(L)\n",
    "    \n",
    "    global optimal_loss\n",
    "    temp_loss = accum_loss\n",
    "    if temp_loss < optimal_loss:\n",
    "        optimal_loss = temp_loss        \n",
    "    print(\"CL Loss: {:.5f}\\tCL Acc: {:.5f}\\tTime: {:.5f}\".format(accum_loss, accum_acc, time.time() - start_time))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Start Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
    "    if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prepare Text Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download SciBert to ../data/pretrained_SciBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "kwargs = {}\n",
    "\n",
    "if args.text_type == \"SciBERT\":\n",
    "    pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
    "    print(\"Download SciBert to {}\".format(pretrained_SciBERT_folder))\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
    "    text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
    "    kwargs[\"text_tokenizer\"] = text_tokenizer\n",
    "    kwargs[\"text_model\"] = text_model\n",
    "    text_dim = 768\n",
    "else:\n",
    "    raise Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Start training MoleculeSTM-SMILES\n",
    "\n",
    "#### 5.3.1 Prepare MegaMolBART (SMILES Model) and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of CID2text: 250962\n",
      "len of CID2SMILES: 250950\n",
      "len of text_list: 1000\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adam_beta1 ...................... 0.9\n",
      "  adam_beta2 ...................... 0.999\n",
      "  adam_eps ........................ 1e-08\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... None\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... False\n",
      "  checkpoint_in_cpu ............... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  contigious_checkpointing ........ False\n",
      "  cpu_optimizer ................... False\n",
      "  cpu_torch_adam .................. False\n",
      "  data_impl ....................... infer\n",
      "  data_path ....................... None\n",
      "  dataset_path .................... None\n",
      "  DDP_impl ........................ local\n",
      "  deepscale ....................... False\n",
      "  deepscale_config ................ None\n",
      "  deepspeed ....................... False\n",
      "  deepspeed_activation_checkpointing  False\n",
      "  deepspeed_config ................ None\n",
      "  deepspeed_mpi ................... False\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 100\n",
      "  exit_interval ................... None\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  gas ............................. 1\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 256\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... None\n",
      "  load ............................ ../data/pretrained_MegaMolBART/checkpoints\n",
      "  local_rank ...................... None\n",
      "  log_interval .................... 100\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. None\n",
      "  lr_decay_iters .................. None\n",
      "  lr_decay_style .................. linear\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 512\n",
      "  merge_file ...................... None\n",
      "  min_lr .......................... 0.0\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 8\n",
      "  num_layers ...................... 4\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... None\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  partition_activations ........... False\n",
      "  pipe_parallel_size .............. 0\n",
      "  profile_backward ................ False\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ None\n",
      "  save_interval ................... None\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... None\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 969, 30, 1\n",
      "  synchronize_each_layer .......... False\n",
      "  tensorboard_dir ................. None\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. GPT2BPETokenizer\n",
      "  train_iters ..................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  vocab_file ...................... ../MoleculeSTM/bart_vocab.txt\n",
      "  warmup .......................... 0.01\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "  zero_allgather_bucket_size ...... 0.0\n",
      "  zero_contigious_gradients ....... False\n",
      "  zero_reduce_bucket_size ......... 0.0\n",
      "  zero_reduce_scatter ............. False\n",
      "  zero_stage ...................... 1.0\n",
      "---------------- end of arguments ----------------\n",
      "> initializing torch distributed ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "Loading vocab from ../MoleculeSTM/bart_vocab.txt.\n",
      "Loading from ../data/pretrained_MegaMolBART/checkpoints\n",
      "global rank 0 is loading checkpoint ../data/pretrained_MegaMolBART/checkpoints/iter_0134000/mp_rank_00/model_optim_rng.pt\n",
      "could not find arguments in the checkpoint ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  successfully loaded ../data/pretrained_MegaMolBART/checkpoints/iter_0134000/mp_rank_00/model_optim_rng.pt\n"
     ]
    }
   ],
   "source": [
    "dataset_root = os.path.join(args.dataspace_path, \"PubChemSTM_data\")\n",
    "    \n",
    "molecule_type = \"SMILES\"\n",
    "\n",
    "dataset = PubChemSTM_SubDatasets_SMILES(dataset_root, size=1000)\n",
    "dataloader_class = torch_DataLoader\n",
    "\n",
    "if args.output_model_dir is not None:\n",
    "    MegaMolBART_dir = os.path.join(args.output_model_dir, \"SMILES\")\n",
    "else:\n",
    "    MegaMolBART_dir = None\n",
    "MegaMolBART_wrapper = MegaMolBART(\n",
    "    vocab_path=\"../MoleculeSTM/bart_vocab.txt\",\n",
    "    input_dir=args.megamolbart_input_dir,\n",
    "    output_dir=MegaMolBART_dir)\n",
    "molecule_model = MegaMolBART_wrapper.model\n",
    "kwargs[\"MegaMolBART_wrapper\"] = MegaMolBART_wrapper\n",
    "kwargs[\"molecule_model\"] = molecule_model\n",
    "molecule_dim = 256\n",
    "\n",
    "dataloader = dataloader_class(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Prepare Two Projection Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text2latent = nn.Linear(text_dim, args.SSL_emb_dim).to(device)\n",
    "mol2latent = nn.Linear(molecule_dim, args.SSL_emb_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Prepare Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param_group = [\n",
    "    {\"params\": text_model.parameters(), \"lr\": args.text_lr},\n",
    "    {\"params\": molecule_model.parameters(), \"lr\": args.mol_lr},\n",
    "    {\"params\": text2latent.parameters(), \"lr\": args.text_lr * args.text_lr_scale},\n",
    "    {\"params\": mol2latent.parameters(), \"lr\": args.mol_lr * args.mol_lr_scale},\n",
    "]\n",
    "optimizer = optim.Adam(model_param_group, weight_decay=args.decay)\n",
    "optimal_loss = 1e10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4 Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:49<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Loss: 0.69800\tCL Acc: 0.50400\tTime: 49.79203\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Loss: 0.69504\tCL Acc: 0.50450\tTime: 48.61034\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:48<00:00,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Loss: 0.69426\tCL Acc: 0.50175\tTime: 48.72926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(3):\n",
    "    print(\"Epoch {}\".format(e))\n",
    "    train(e, dataloader, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
