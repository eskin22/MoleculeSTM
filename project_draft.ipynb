{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is a draft of the in-progress reproduction of the paper \"Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing\" originally authored by Liu et al. (2023).\n",
    "\n",
    "In this section we provide background on the purpose of this research, as well as an overview of the approach concerning structural and textual representation of molecules, as well as the criticality of pretraining for the MoleculeSTM modal.\n",
    "\n",
    "Please refer to the orginal source code from the paper [here](https://github.com/chao1224/MoleculeSTM), and my own GitHub repo which has been modified for this project [here](https://github.com/eskin22/MoleculeSTM_DLH)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Drug discovery is a complex, time-consuming, and costly process that involves identifying molecules with therapeutic potential. To that point, recent progress in artificial intelligence (AI) offers a way to minimize these aspects and transform the process as a whole. \n",
    "\n",
    "The conventional approach for AI-facilitated drug discovery relies heavily on understanding the chemical structures of molecules, often overlooking the vast, unstructured textual knowledge available, which could offer insights into new drug design objectives, adaptability to text-based instructions, and predictions of complex biological activities. Indeed, Liu et al. (2023) breathe life into this notion with their introduction of MoleculeSTM, a multi-modal molecule structure-text modal that leverages both molecular structural information and textual descriptions through a contrastive learning strategy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural and Textual Representation of Molecules\n",
    "\n",
    "MoleculeSTM represents a multi-modal foundation model that concurrently learns from molecules' chemical structures and their associated textual descriptions. This dual-branch approach, incorporating a chemical structure branch and a textual description branch, allows for the integration of existing molecular structural models and scientific language models (Liu et al., 2023). The contrastive learning paradigm bridges these two branches, facilitating a comprehensive understanding of molecular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticality of Pretraining\n",
    "\n",
    "To enable training for the model given the two distinct branches described, the authors have synthesized this data into a structure-text dataset; in which each chemical structure is associated with a textual description. The open-ended format of the textual descriptions allow the model to achieve better zero-shot performance i.e. generalize to unseen data (Liu et al., 2023). Using the PubChemSTM dataset, the authors pretrain the model via contrastive learning. The general framework is to map representations from the structural and textual branches to a shared molecular model by reducing the distance between pairs of the same molecule and decreasing that between pairs for different molecules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of Reproducability\n",
    "\n",
    "### A Note On This Notebook\n",
    "\n",
    "Given the complexity of this project, it is unfortunately a very prohibitive process to migrate the code, as well as the environmental dependencies, into a Google Colab notebook. To the extent that this draft is intended to reflect progress made on this project, we will save time attempting to make all of this code fit into that format in favor on making real progress towards the end goal of reproducing results. \n",
    "\n",
    "### Hypotheses\n",
    "\n",
    "Here we list out the hypotheses from the original paper that we will be testing for our reproduction.\n",
    "\n",
    "#### Zero-shot structure-text retrieval\n",
    "\n",
    "MoleculeSTM can accurately link molecular structures to their textual descriptions without being directly trained on these specific pairs, showcasing its ability to generalize from its pretraining.\n",
    "\n",
    "#### Zero-shot text-based molecule editing\n",
    "\n",
    "MoleculeSTM can edit and generate molecular structures to meet new specifications provided via textual descriptions, demonstrating its understanding of complex chemical properties and functionalities from text alone.\n",
    "\n",
    "#### Molecular property prediction\n",
    "\n",
    "MoleculeSTM can predict molecular properties accurately, leveraging its pretraining on structure-text relationships, suggesting that it captures meaningful chemical information that is relevant for property prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we do our best to provide updated setup instructions for this project adapted from the Shangchao Liu's recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OS Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my own testing, the only operating system that currently works for MoleculeSTM is only workable on a Linux system. Do not even try to do this on Windows. MacOS will only work if you have a plan for using some alternate strategy for pretraining, as most Macs lack sufficient GPUs to perform this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Next, you'll want to ensure that you can install all of the following dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n MoleculeSTM python=3.7\n",
    "conda activate MoleculeSTM\n",
    "\n",
    "conda install -y -c rdkit rdkit=2020.09.1.0\n",
    "conda install -y -c conda-forge -c pytorch pytorch=1.9.1\n",
    "conda install -y -c pyg -c conda-forge pyg==2.0.3\n",
    "\n",
    "pip install requests\n",
    "pip install tqdm\n",
    "pip install matplotlib\n",
    "pip install spacy\n",
    "pip install Levenshtein\n",
    "\n",
    "# for SciBert\n",
    "conda install -y boto3\n",
    "pip install transformers\n",
    "\n",
    "# for MoleculeNet\n",
    "pip install ogb==1.2.0\n",
    "\n",
    "# install pysmilesutils\n",
    "python -m pip install git+https://github.com/MolecularAI/pysmilesutils.git\n",
    "\n",
    "pip install deepspeed\n",
    "\n",
    "# install metagron\n",
    "# pip install megatron-lm==1.1.5\n",
    "git clone https://github.com/MolecularAI/MolBART.git --branch megatron-molbart-with-zinc\n",
    "cd MolBART/megatron_molbart/Megatron-LM-v1.1.5-3D_parallelism\n",
    "pip install .\n",
    "cd ../../..\n",
    "\n",
    "# install apex\n",
    "# wget https://github.com/NVIDIA/apex/archive/refs/tags/22.03.zip\n",
    "# unzip 22.03.zip\n",
    "git clone https://github.com/chao1224/apex.git\n",
    "cd apex\n",
    "pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we provide an overview of the methodology of this project, including a description of the data with included code for acquisition and preprocessing, as well as the model's architecture and pretraining approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the massive size of the data, as well as the need to keep the various datasets aligned with each other for the purposes of pretraining, we are unable to push the datasets with the limitatioms of pushes to a GitHub repository, nor can we simply upload a sample of the data. However, we will provide the link to download the data from HuggingFace [here](https://huggingface.co/datasets/chao1224/MoleculeSTM), similarly to how we accessed the data for ourselves. In addition, we provide this script adapted from the original code to download all the required training and downstream datasets that will be used for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "api = HfApi()\n",
    "snapshot_download(repo_id=\"chao1224/MoleculeSTM\", repo_type=\"dataset\", local_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to this, there is a required step to perform preprocessing on the PubChemSTM dataset; this is due to the fact that the PubChemSTM dataset only has the license to share the structural representations of molecules, but not the textual represetntation. So, in order to complete the dataset required for pretraining the model, you must follow the instructions below adapted from the advice of Shangchao Liu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Description Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "def clean_up_description(description):\n",
    "    description = description + \" \"\n",
    "\n",
    "    ##### extra adj Pure #####\n",
    "    if description.startswith(\"Pure \"):\n",
    "        description = description.replace(\"Pure \", \"\")\n",
    "    ##### fix typo #####\n",
    "    if description.startswith(\"Mercurycombines\"):\n",
    "        description = description.replace(\"Mercurycombines\", \"Mercury combines\")\n",
    "    \n",
    "    name_special_case_list = [\n",
    "        '17-Hydroxy-6-methylpregna-3,6-diene-3,20-dione. ',\n",
    "        '5-Thymidylic acid. ',\n",
    "        \"5'-S-(3-Amino-3-carboxypropyl)-5'-thioadenosine. \",\n",
    "        \"Guanosine 5'-(trihydrogen diphosphate), monoanhydride with phosphorothioic acid. \",\n",
    "        \"5'-Uridylic acid. \",\n",
    "        \"5'-Adenylic acid, \",\n",
    "        \"Uridine 5'-(tetrahydrogen triphosphate). \",\n",
    "        \"Inosine 5'-Monophosphate. \",\n",
    "        \"Pivaloyloxymethyl butyrate (AN-9), \",\n",
    "        \"4-Amino-5-cyano-7-(D-ribofuranosyl)-7H- pyrrolo(2,3-d)pyrimidine. \",\n",
    "        \"Cardamonin (also known as Dihydroxymethoxychalcone), \",\n",
    "    ]\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"17-Hydroxy-6-methylpregna-3,6-diene-3,20-dione. \", \"17-Hydroxy-6-methylpregna-3,6-diene-3,20-dione is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"5-Thymidylic acid. \", \"5-Thymidylic acid. is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"5'-S-(3-Amino-3-carboxypropyl)-5'-thioadenosine. \", \"5'-S-(3-Amino-3-carboxypropyl)-5'-thioadenosine. is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Guanosine 5'-(trihydrogen diphosphate), monoanhydride with phosphorothioic acid. \", \"Guanosine 5'-(trihydrogen diphosphate), monoanhydride with phosphorothioic acid is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"5'-Uridylic acid. \", \"5'-Uridylic acid is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"5'-Adenylic acid, \", \"5'-Adenylic acid is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Uridine 5'-(tetrahydrogen triphosphate). \", \"Uridine 5'-(tetrahydrogen triphosphate). is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Inosine 5'-Monophosphate. \", \"Inosine 5'-Monophosphate. is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Pivaloyloxymethyl butyrate (AN-9), \", \"Pivaloyloxymethyl butyrate (AN-9) is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"4-Amino-5-cyano-7-(D-ribofuranosyl)-7H- pyrrolo(2,3-d)pyrimidine. \", \"4-Amino-5-cyano-7-(D-ribofuranosyl)-7H- pyrrolo(2,3-d)pyrimidine is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Cardamonin (also known as Dihydroxymethoxychalcone), \", \"Cardamonin (also known as Dihydroxymethoxychalcone) is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Lithium has been used to treat \", \"Lithium is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"4,4'-Methylenebis \", \"4,4'-Methylenebis is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"2,3,7,8-Tetrachlorodibenzo-p-dioxin\", \"2,3,7,8-Tetrachlorodibenzo-p-dioxin is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Exposure to 2,4,5-trichlorophenol \", \"2,4,5-Trichlorophenol exposure \")\n",
    "\n",
    "    index = 0\n",
    "    L = len(description)\n",
    "    if description.startswith('C.I. '):\n",
    "        start_index = len('C.I. ')\n",
    "    elif description.startswith('Nectriapyrone. D '):\n",
    "        start_index = len('Nectriapyrone. D ')\n",
    "    elif description.startswith('Salmonella enterica sv. Minnesota LPS core oligosaccharide'):\n",
    "        start_index = len('Salmonella enterica sv. Minnesota LPS core oligosaccharide')\n",
    "    else:\n",
    "        start_index = 0\n",
    "    for index in range(start_index, L - 1):\n",
    "        if index < L-2:\n",
    "            if description[index] == '.' and description[index+1] == ' ' and 'A' <= description[index+2] <= 'Z':\n",
    "                break\n",
    "        elif index == L - 2:\n",
    "            break\n",
    "    \n",
    "    first_sentence = description[:index+1]\n",
    "    return first_sentence\n",
    "\n",
    "\n",
    "def extract_name(name_raw, description):\n",
    "    first_sentence = clean_up_description(description)\n",
    "\n",
    "    splitter = '  --  --  '\n",
    "    if ' are ' in first_sentence or ' were ' in first_sentence:\n",
    "        replaced_words = 'These molecules'\n",
    "    else:\n",
    "        replaced_words = 'This molecule'\n",
    "\n",
    "    first_sentence = first_sentence.replace(' is ', splitter)\n",
    "    first_sentence = first_sentence.replace(' are ', splitter)\n",
    "    first_sentence = first_sentence.replace(' was ', splitter)\n",
    "    first_sentence = first_sentence.replace(' were ', splitter)\n",
    "    first_sentence = first_sentence.replace(' appears ', splitter)\n",
    "    first_sentence = first_sentence.replace(' occurs ', splitter)\n",
    "    first_sentence = first_sentence.replace(' stands for ', splitter)\n",
    "    first_sentence = first_sentence.replace(' belongs to ', splitter)\n",
    "    first_sentence = first_sentence.replace(' exists ', splitter) # only for CID=11443\n",
    "    first_sentence = first_sentence.replace(' has been used in trials ', splitter)\n",
    "    first_sentence = first_sentence.replace(' has been investigated ', splitter)\n",
    "    first_sentence = first_sentence.replace(' has many uses ', splitter)\n",
    "    \n",
    "    if splitter in first_sentence:\n",
    "        extracted_name = first_sentence.split(splitter, 1)[0]\n",
    "    elif first_sentence.startswith(name_raw):\n",
    "        extracted_name = name_raw\n",
    "    elif name_raw in first_sentence:\n",
    "        extracted_name = name_raw\n",
    "        extracted_name = None\n",
    "        print(\"=====\", name_raw)\n",
    "        print(\"first sentence: \", first_sentence)\n",
    "        # print()\n",
    "    else:\n",
    "        extracted_name = None\n",
    "\n",
    "    if extracted_name is not None:\n",
    "        extracted_description = description.replace(extracted_name, replaced_words)\n",
    "    else:\n",
    "        extracted_description = description\n",
    "\n",
    "    return extracted_name, extracted_description, first_sentence\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_page_num = 290\n",
    "    # Please put your own dataset path here\n",
    "    datasets_home_folder = \"../../../Datasets\"\n",
    "\n",
    "    PubChemSTM_datasets_description_home_folder = \"{}/step_01_PubChemSTM_description\".format(datasets_home_folder)\n",
    "    valid_CID_list = set()\n",
    "    CID2name_raw, CID2name_extracted = defaultdict(list), defaultdict(list)\n",
    "    CID2text_raw, CID2text_extracted = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for page_index in tqdm(range(total_page_num)):\n",
    "        page_num = page_index + 1\n",
    "        compound_description_file_name = \"Compound_description_{}.txt\".format(page_num)\n",
    "        f_out = open(\"{}/{}\".format(PubChemSTM_datasets_description_home_folder, compound_description_file_name), \"w\")\n",
    "        \n",
    "        description_url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/annotations/heading/json?heading_type=Compound&heading=Record+Description&page={}\".format(page_num)\n",
    "        description_data = requests.get(description_url).json()\n",
    "\n",
    "        description_data = description_data[\"Annotations\"]\n",
    "        assert description_data[\"Page\"] == page_num\n",
    "        assert description_data[\"TotalPages\"] == total_page_num\n",
    "        \n",
    "        record_list = description_data[\"Annotation\"]\n",
    "        \n",
    "        for record in record_list:\n",
    "            try:\n",
    "                CID = record[\"LinkedRecords\"][\"CID\"][0]\n",
    "                if \"Name\" in record:\n",
    "                    name_raw = record[\"Name\"]\n",
    "                    CID2name_raw[CID].append(name_raw)\n",
    "                else:\n",
    "                    name_raw = None\n",
    "\n",
    "                data_list = record[\"Data\"]\n",
    "                for data in data_list:\n",
    "                    description = data[\"Value\"][\"StringWithMarkup\"][0][\"String\"].strip()\n",
    "                    \n",
    "                    extracted_name, extracted_description, first_sentence = extract_name(name_raw, description)\n",
    "                    if extracted_name is not None:\n",
    "                        CID2name_extracted[CID].append(extracted_name)\n",
    "\n",
    "                    CID_special_case_list = [45266824, 11683, 3759, 9700, 439155, 135398675, 135563708, 6030, 10238, 6133, 135398640, 77918, 60748, 11824, 641785, 11125, 7543, 15625, 7271]\n",
    "\n",
    "                    ##### only for debugging #####\n",
    "                    if CID in CID_special_case_list:\n",
    "                        print(\"page: {}\\tCID: {}\".format(page_index, CID))                \n",
    "                        if \"Name\" in record:\n",
    "                            print('yes-name')\n",
    "                            name = record[\"Name\"]\n",
    "                            print('name:', name)\n",
    "                        else:\n",
    "                            print('no-name')\n",
    "                        print('extracted name:', extracted_name)\n",
    "                        print(\"first_sentence:\", first_sentence)\n",
    "                        print(\"extracted_description:\", extracted_description)\n",
    "                        print(\"description:\", description)\n",
    "                        print() \n",
    "                        \n",
    "                    CID2text_raw[CID].append(description)\n",
    "                    CID2text_extracted[CID].append(extracted_description)\n",
    "\n",
    "                    valid_CID_list.add(CID)\n",
    "                    f_out.write(\"{}\\n\".format(CID))\n",
    "                    f_out.write(\"{}\\n\\n\".format(extracted_description))\n",
    "            except:\n",
    "                # print(\"===\\n\", record)\n",
    "                # print(\"missing page: {}\\tSourceName: {}\\tSourceID: {}\".format(page_index, record['SourceName'], record['SourceID']))\n",
    "                continue\n",
    "            \n",
    "    valid_CID_list = list(set(valid_CID_list))\n",
    "    valid_CID_list = sorted(valid_CID_list)\n",
    "    # print(\"valid CID list: {}\".format(valid_CID_list))\n",
    "    print(\"Total CID (with raw name) {}\".format(len(CID2name_raw)))\n",
    "    print(\"Total CID (with extracted name) {}\".format(len(CID2name_extracted)))\n",
    "    print(\"Total CID {}\".format(len(valid_CID_list)))\n",
    "    \n",
    "    with open(\"{}/PubChemSTM_data/raw/CID2name_raw.json\".format(datasets_home_folder), \"w\") as f:\n",
    "        json.dump(CID2name_raw, f)\n",
    "    \n",
    "    with open(\"{}/PubChemSTM_data/raw/CID2name.json\".format(datasets_home_folder), \"w\") as f:\n",
    "        json.dump(CID2name_extracted, f)\n",
    "\n",
    "    with open(\"{}/PubChemSTM_data/raw/CID2text_raw.json\".format(datasets_home_folder), \"w\") as f:\n",
    "        json.dump(CID2text_raw, f)\n",
    "\n",
    "    with open(\"{}/PubChemSTM_data/raw/CID2text.json\".format(datasets_home_folder), \"w\") as f:\n",
    "        json.dump(CID2text_extracted, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download SDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from PubChem_utils import download_and_extract_compound_file\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--block_id\", type=int, default=0)\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    datasets_home_folder = \"../../../Datasets\"\n",
    "\n",
    "    PubChemSTM_datasets_home_folder = \"{}/step_02_PubChemSTM_SDF\".format(datasets_home_folder)\n",
    "    block_id = args.block_id\n",
    "    block_size = 500000\n",
    "    start_id = block_id * block_size + 1\n",
    "    end_id = (block_id + 1) * block_size\n",
    "\n",
    "    compound_file_name = \"Compound_{:09d}_{:09d}.sdf.gz\".format(start_id, end_id)\n",
    "    download_and_extract_compound_file(PubChemSTM_datasets_home_folder, compound_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Filter Out SDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    datasets_home_folder = \"../../../Datasets\"\n",
    "\n",
    "    PubChemSTM_datasets_description_home_folder = \"{}/step_01_PubChemSTM_description\".format(datasets_home_folder)\n",
    "    with open(\"{}/PubChemSTM_data/raw/CID2text.json\".format(datasets_home_folder), \"r\") as f:\n",
    "        CID2text = json.load(f)\n",
    "    target_CID_list = set(CID2text.keys())\n",
    "\n",
    "    PubChemSTM_datasets_input_folder = \"{}/step_02_PubChemSTM_SDF\".format(datasets_home_folder)\n",
    "    PubChemSTM_datasets_output_folder = \"{}/step_03_PubChemSTM_filtered\".format(datasets_home_folder)\n",
    "    block_size = 500000\n",
    "\n",
    "    def extract_one_SDF_file(block_id):\n",
    "        valid_mol_count = 0\n",
    "\n",
    "        writer = Chem.SDWriter('{}/filtered_{}.sdf'.format(PubChemSTM_datasets_output_folder, block_id))\n",
    "        start_id = block_id * block_size + 1\n",
    "        end_id = (block_id + 1) * block_size\n",
    "\n",
    "        compound_file_name = \"Compound_{:09d}_{:09d}.sdf.gz\".format(start_id, end_id)\n",
    "        gzip_loader = gzip.open(\"{}/{}\".format(PubChemSTM_datasets_input_folder, compound_file_name))\n",
    "        suppl = Chem.ForwardSDMolSupplier(gzip_loader)\n",
    "\n",
    "        for mol in tqdm(suppl):\n",
    "            if mol is None:\n",
    "                continue\n",
    "            cid = mol.GetProp(\"PUBCHEM_COMPOUND_CID\")\n",
    "\n",
    "            if cid not in target_CID_list:\n",
    "                continue\n",
    "\n",
    "            writer.write(mol)\n",
    "            valid_mol_count += 1\n",
    "\n",
    "        print(\"block id: {}\\nfound {}\\n\\n\".format(block_id, valid_mol_count))\n",
    "        sys.stdout.flush()\n",
    "        return\n",
    "    \n",
    "    num_process = multiprocessing.cpu_count()\n",
    "    print(\"{} CPUs\".format(num_process))\n",
    "    num_process = 8\n",
    "    p = Pool(num_process)\n",
    "\n",
    "    total_block_num = 325\n",
    "    block_id_list = np.arange(total_block_num+1)\n",
    "    with p:\n",
    "        p.map(extract_one_SDF_file, block_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Merge SDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    datasets_home_folder = \"../../../Datasets\"\n",
    "\n",
    "    PubChemSTM_datasets_description_home_folder = \"{}/step_01_PubChemSTM_description\".format(datasets_home_folder)\n",
    "    with open(\"{}/PubChemSTM_data/raw/CID2text.json\".format(datasets_home_folder), \"r\") as f:\n",
    "        CID2text = json.load(f)\n",
    "    target_CID_list = set(CID2text.keys())\n",
    "    print('The length of target_CID_list: {}'.format(len(target_CID_list)))\n",
    "\n",
    "    PubChemSTM_datasets_folder = \"{}/step_03_PubChemSTM_filtered\".format(datasets_home_folder)\n",
    "    writer = Chem.SDWriter('{}/PubChemSTM_data/raw/molecules.sdf'.format(datasets_home_folder))\n",
    "    \n",
    "    total_block_num = 325\n",
    "    found_CID_set = set()\n",
    "    for block_id in range(total_block_num+1):\n",
    "        compound_file_path = \"{}/filtered_{}.sdf\".format(PubChemSTM_datasets_folder, block_id)\n",
    "        try:\n",
    "            suppl = Chem.SDMolSupplier(compound_file_path)\n",
    "\n",
    "            for mol in tqdm(suppl):\n",
    "                writer.write(mol)\n",
    "                cid = mol.GetProp(\"PUBCHEM_COMPOUND_CID\")\n",
    "                found_CID_set.add(cid)\n",
    "        except:\n",
    "            print(\"block id: {} with 0 valid SDF file\".format(block_id))\n",
    "            continue\n",
    "\n",
    "    for CID in target_CID_list:\n",
    "        if CID not in found_CID_set:\n",
    "            print(\"CID: {} not found.\".format(CID))\n",
    "    \n",
    "    print(\"In total: {} molecules\".format(len(found_CID_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Sample Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    datasets_home_folder = \"../../../Datasets\"\n",
    "\n",
    "    PubChemSTM_datasets_description_home_folder = \"{}/step_01_PubChemSTM_description\".format(datasets_home_folder)\n",
    "    with open(\"{}/PubChemSTM_data/raw/CID2text.json\".format(datasets_home_folder), \"r\") as f:\n",
    "        CID2text = json.load(f)\n",
    "    target_CID_list = set(CID2text.keys())\n",
    "    print('The length of target_CID_list: {}'.format(len(target_CID_list)))\n",
    "\n",
    "    PubChemSTM_datasets_folder = \"{}/step_03_PubChemSTM_filtered\".format(datasets_home_folder)\n",
    "    writer = Chem.SDWriter('{}/PubChemSTM_data/raw/molecules.sdf'.format(datasets_home_folder))\n",
    "    \n",
    "    total_block_num = 325\n",
    "    found_CID_set = set()\n",
    "    for block_id in range(total_block_num+1):\n",
    "        compound_file_path = \"{}/filtered_{}.sdf\".format(PubChemSTM_datasets_folder, block_id)\n",
    "        try:\n",
    "            suppl = Chem.SDMolSupplier(compound_file_path)\n",
    "\n",
    "            for mol in tqdm(suppl):\n",
    "                writer.write(mol)\n",
    "                cid = mol.GetProp(\"PUBCHEM_COMPOUND_CID\")\n",
    "                found_CID_set.add(cid)\n",
    "        except:\n",
    "            print(\"block id: {} with 0 valid SDF file\".format(block_id))\n",
    "            continue\n",
    "\n",
    "    for CID in target_CID_list:\n",
    "        if CID not in found_CID_set:\n",
    "            print(\"CID: {} not found.\".format(CID))\n",
    "    \n",
    "    print(\"In total: {} molecules\".format(len(found_CID_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, we include the code from the original paper for the architecure and pretraining of the MoleculeSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by including the architecture for MoleculeSTM, which uses a contrastive pretraining approach and a number of linear layers corresponding to the `[input_dim] + hidden_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from collections.abc import Sequence\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, batch_norm=False, activation=\"relu\", dropout=0):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        if not isinstance(hidden_dims, Sequence):\n",
    "            hidden_dims = [hidden_dims]\n",
    "        self.dims = [input_dim] + hidden_dims\n",
    "\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(self.dims) - 1):\n",
    "            self.layers.append(nn.Linear(self.dims[i], self.dims[i + 1]))\n",
    "        if batch_norm:\n",
    "            self.batch_norms = nn.ModuleList()\n",
    "            for i in range(len(self.dims) - 2):\n",
    "                self.batch_norms.append(nn.BatchNorm1d(self.dims[i + 1]))\n",
    "        else:\n",
    "            self.batch_norms = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        layer_input = input\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            hidden = layer(layer_input)\n",
    "            if i < len(self.layers) - 1:\n",
    "                if self.batch_norms:\n",
    "                    x = hidden.flatten(0, -2)\n",
    "                    hidden = self.batch_norms[i](x).view_as(hidden)\n",
    "                hidden = self.activation(hidden)\n",
    "                if self.dropout:\n",
    "                    hidden = self.dropout(hidden)\n",
    "            if hidden.shape == layer_input.shape:\n",
    "                hidden = hidden + layer_input\n",
    "            layer_input = hidden\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide the script used for pretraining the MoleculeSTM model. This script synthesizes the datasets after preprocessing and utilizes a contrastive learning approach. The general framework is to map representations from the structural and textual branches to a shared molecular model by reducing the distance between pairs of the same molecule and decreasing that between pairs for different molecules. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader as torch_DataLoader\n",
    "\n",
    "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from MoleculeSTM.datasets import (\n",
    "    PubChemSTM_Datasets_SMILES, PubChemSTM_SubDatasets_SMILES,\n",
    "    PubChemSTM_Datasets_Graph, PubChemSTM_SubDatasets_Graph,\n",
    "    PubChemSTM_Datasets_Raw_SMILES, PubChemSTM_SubDatasets_Raw_SMILES,\n",
    "    PubChemSTM_Datasets_Raw_Graph, PubChemSTM_SubDatasets_Raw_Graph\n",
    ")\n",
    "from MoleculeSTM.models import GNN, GNN_graphpred\n",
    "from MoleculeSTM.utils import prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network\n",
    "from MoleculeSTM.models.mega_molbart.mega_mol_bart import MegaMolBART\n",
    "\n",
    "\n",
    "def cycle_index(num, shift):\n",
    "    arr = torch.arange(num) + shift\n",
    "    arr[-shift:] = torch.arange(shift)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def do_CL(X, Y, args):\n",
    "    if args.normalize:\n",
    "        X = F.normalize(X, dim=-1)\n",
    "        Y = F.normalize(Y, dim=-1)\n",
    "\n",
    "    if args.SSL_loss == 'EBM_NCE':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        neg_Y = torch.cat([Y[cycle_index(len(Y), i + 1)] for i in range(args.CL_neg_samples)], dim=0)\n",
    "        neg_X = X.repeat((args.CL_neg_samples, 1))\n",
    "\n",
    "        pred_pos = torch.sum(X * Y, dim=1) / args.T\n",
    "        pred_neg = torch.sum(neg_X * neg_Y, dim=1) / args.T\n",
    "\n",
    "        loss_pos = criterion(pred_pos, torch.ones(len(pred_pos)).to(pred_pos.device))\n",
    "        loss_neg = criterion(pred_neg, torch.zeros(len(pred_neg)).to(pred_neg.device))\n",
    "        CL_loss = (loss_pos + args.CL_neg_samples * loss_neg) / (1 + args.CL_neg_samples)\n",
    "\n",
    "        CL_acc = (torch.sum(pred_pos > 0).float() + torch.sum(pred_neg < 0).float()) / \\\n",
    "                 (len(pred_pos) + len(pred_neg))\n",
    "        CL_acc = CL_acc.detach().cpu().item()\n",
    "\n",
    "    elif args.SSL_loss == 'InfoNCE':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        B = X.size()[0]\n",
    "        logits = torch.mm(X, Y.transpose(1, 0))  # B*B\n",
    "        logits = torch.div(logits, args.T)\n",
    "        labels = torch.arange(B).long().to(logits.device)  # B*1\n",
    "\n",
    "        CL_loss = criterion(logits, labels)\n",
    "        pred = logits.argmax(dim=1, keepdim=False)\n",
    "        CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
    "\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "    return CL_loss, CL_acc\n",
    "\n",
    "\n",
    "def save_model(save_best, epoch=None):\n",
    "    if args.output_model_dir is not None:\n",
    "        if save_best:\n",
    "            global optimal_loss\n",
    "            print(\"save model with loss: {:.5f}\".format(optimal_loss))\n",
    "            model_file = \"model.pth\"\n",
    "\n",
    "        elif epoch is None:\n",
    "            model_file = \"model_final.pth\"\n",
    "\n",
    "        else:\n",
    "            model_file = \"model_{}.pth\".format(epoch)\n",
    "\n",
    "        saved_file_path = os.path.join(args.output_model_dir, \"text_{}\".format(model_file))\n",
    "        torch.save(text_model.state_dict(), saved_file_path)\n",
    "\n",
    "        saved_file_path = os.path.join(args.output_model_dir, \"molecule_{}\".format(model_file))\n",
    "        torch.save(molecule_model.state_dict(), saved_file_path)\n",
    "        \n",
    "        saved_file_path = os.path.join(args.output_model_dir, \"text2latent_{}\".format(model_file))\n",
    "        torch.save(text2latent.state_dict(), saved_file_path)\n",
    "        \n",
    "        saved_file_path = os.path.join(args.output_model_dir, \"mol2latent_{}\".format(model_file))\n",
    "        torch.save(mol2latent.state_dict(), saved_file_path)\n",
    "    return\n",
    "\n",
    "\n",
    "def train(\n",
    "    epoch,\n",
    "    dataloader,\n",
    "    text_model, text_tokenizer,\n",
    "    molecule_model, MegaMolBART_wrapper=None):\n",
    "\n",
    "    if args.representation_frozen:\n",
    "        text_model.eval()\n",
    "        molecule_model.eval()\n",
    "    else:\n",
    "        text_model.train()\n",
    "        molecule_model.train()\n",
    "    text2latent.train()\n",
    "    mol2latent.train()\n",
    "\n",
    "    if args.verbose:\n",
    "        L = tqdm(dataloader)\n",
    "    else:\n",
    "        L = dataloader\n",
    "    \n",
    "    start_time = time.time()\n",
    "    accum_loss, accum_acc = 0, 0\n",
    "    for step, batch in enumerate(L):\n",
    "        description = batch[0]\n",
    "        molecule_data = batch[1]\n",
    "\n",
    "        description_tokens_ids, description_masks = prepare_text_tokens(\n",
    "            device=device, description=description, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
    "        description_output = text_model(input_ids=description_tokens_ids, attention_mask=description_masks)\n",
    "        description_repr = description_output[\"pooler_output\"]\n",
    "        description_repr = text2latent(description_repr)\n",
    "\n",
    "        if molecule_type == \"SMILES\":\n",
    "            molecule_data = list(molecule_data) # for SMILES_list\n",
    "            molecule_repr = get_molecule_repr_MoleculeSTM(\n",
    "                molecule_data, mol2latent=mol2latent,\n",
    "                molecule_type=molecule_type, MegaMolBART_wrapper=MegaMolBART_wrapper)\n",
    "        else:\n",
    "            molecule_data = molecule_data.to(device)\n",
    "            molecule_repr = get_molecule_repr_MoleculeSTM(\n",
    "                molecule_data, mol2latent=mol2latent,\n",
    "                molecule_type=molecule_type, molecule_model=molecule_model)\n",
    "        \n",
    "        loss_01, acc_01 = do_CL(description_repr, molecule_repr, args)\n",
    "        loss_02, acc_02 = do_CL(molecule_repr, description_repr, args)\n",
    "        loss = (loss_01 + loss_02) / 2\n",
    "        acc = (acc_01 + acc_02) / 2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        accum_loss += loss.item()\n",
    "        accum_acc += acc\n",
    "    \n",
    "    accum_loss /= len(L)\n",
    "    accum_acc /= len(L)\n",
    "    \n",
    "    global optimal_loss\n",
    "    temp_loss = accum_loss\n",
    "    if temp_loss < optimal_loss:\n",
    "        optimal_loss = temp_loss\n",
    "        save_model(save_best=True, epoch=epoch)\n",
    "    print(\"CL Loss: {:.5f}\\tCL Acc: {:.5f}\\tTime: {:.5f}\".format(accum_loss, accum_acc, time.time() - start_time))\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--device\", type=int, default=0)\n",
    "\n",
    "    parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"PubChemSTM\")\n",
    "    parser.add_argument(\"--text_type\", type=str, default=\"SciBERT\", choices=[\"SciBERT\"])\n",
    "    parser.add_argument(\"--molecule_type\", type=str, default=\"SMILES\", choices=[\"SMILES\", \"Graph\"])\n",
    "    parser.add_argument(\"--representation_frozen\", dest='representation_frozen', action='store_true')\n",
    "    parser.add_argument('--no_representation_frozen', dest='representation_frozen', action='store_false')\n",
    "    parser.set_defaults(representation_frozen=False)\n",
    "\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--text_lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--mol_lr\", type=float, default=1e-5)\n",
    "    parser.add_argument(\"--text_lr_scale\", type=float, default=1)\n",
    "    parser.add_argument(\"--mol_lr_scale\", type=float, default=1)\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "    parser.add_argument(\"--decay\", type=float, default=0)\n",
    "    parser.add_argument('--verbose', dest='verbose', action='store_true')\n",
    "    parser.set_defaults(verbose=False)\n",
    "    parser.add_argument(\"--output_model_dir\", type=str, default=None)\n",
    "\n",
    "    ########## for SciBERT ##########\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
    "\n",
    "    ########## for MegaMolBART ##########\n",
    "    parser.add_argument(\"--megamolbart_input_dir\", type=str, default=\"../data/pretrained_MegaMolBART/checkpoints\")\n",
    "    parser.add_argument(\"--vocab_path\", type=str, default=\"../MoleculeSTM/bart_vocab.txt\")\n",
    "\n",
    "    ########## for 2D GNN ##########\n",
    "    parser.add_argument(\"--pretrain_gnn_mode\", type=str, default=\"GraphMVP_G\", choices=[\"GraphMVP_G\"])\n",
    "    parser.add_argument(\"--gnn_emb_dim\", type=int, default=300)\n",
    "    parser.add_argument(\"--num_layer\", type=int, default=5)\n",
    "    parser.add_argument('--JK', type=str, default='last')\n",
    "    parser.add_argument(\"--dropout_ratio\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--gnn_type\", type=str, default=\"gin\")\n",
    "    parser.add_argument('--graph_pooling', type=str, default='mean')\n",
    "\n",
    "    ########## for contrastive SSL ##########\n",
    "    parser.add_argument(\"--SSL_loss\", type=str, default=\"EBM_NCE\", choices=[\"EBM_NCE\", \"InfoNCE\"])\n",
    "    parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
    "    parser.add_argument(\"--CL_neg_samples\", type=int, default=1)\n",
    "    parser.add_argument(\"--T\", type=float, default=0.1)\n",
    "    parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
    "    parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
    "    parser.set_defaults(normalize=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(\"arguments\\t\", args)\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    device = torch.device(\"cuda:\" + str(args.device)) \\\n",
    "        if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    if \"PubChemSTM\" in args.dataset:\n",
    "        dataset_root = os.path.join(args.dataspace_path, \"PubChemSTM_data\")\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "    kwargs = {}\n",
    "\n",
    "    # ##### prepare text model #####\n",
    "    if args.text_type == \"SciBERT\":\n",
    "        pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
    "        text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
    "        text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
    "        kwargs[\"text_tokenizer\"] = text_tokenizer\n",
    "        kwargs[\"text_model\"] = text_model\n",
    "        text_dim = 768\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "    ##### prepare molecule model #####\n",
    "    molecule_type = args.molecule_type\n",
    "    if molecule_type == \"SMILES\":\n",
    "        if args.dataset == \"PubChemSTM\":\n",
    "            dataset = PubChemSTM_Datasets_SMILES(dataset_root)\n",
    "        elif args.dataset == \"PubChemSTM1K\":\n",
    "            # only for testing\n",
    "            dataset = PubChemSTM_SubDatasets_SMILES(dataset_root, size=1000)\n",
    "        elif args.dataset == \"PubChemSTM_Raw\":\n",
    "            dataset = PubChemSTM_Datasets_Raw_SMILES(dataset_root)\n",
    "        elif args.dataset == \"PubChemSTM10K_Raw\":\n",
    "            # only for testing\n",
    "            dataset = PubChemSTM_SubDatasets_Raw_SMILES(dataset_root, size=10000)\n",
    "        else:\n",
    "            raise Exception\n",
    "        dataloader_class = torch_DataLoader\n",
    "\n",
    "        if args.output_model_dir is not None:\n",
    "            MegaMolBART_dir = os.path.join(args.output_model_dir, \"MegaMolBART\")\n",
    "        else:\n",
    "            MegaMolBART_dir = None\n",
    "        MegaMolBART_wrapper = MegaMolBART(\n",
    "            vocab_path=args.vocab_path,\n",
    "            input_dir=args.megamolbart_input_dir,\n",
    "            output_dir=MegaMolBART_dir)\n",
    "        molecule_model = MegaMolBART_wrapper.model\n",
    "        kwargs[\"MegaMolBART_wrapper\"] = MegaMolBART_wrapper\n",
    "        kwargs[\"molecule_model\"] = molecule_model\n",
    "        molecule_dim = 256\n",
    "\n",
    "    elif molecule_type == \"Graph\":\n",
    "        if args.dataset == \"PubChemSTM\":\n",
    "            dataset = PubChemSTM_Datasets_Graph(dataset_root)\n",
    "        elif args.dataset == \"PubChemSTM1K\":\n",
    "            dataset = PubChemSTM_SubDatasets_Graph(dataset_root, size=1000)\n",
    "        elif args.dataset == \"PubChemSTM10K\":\n",
    "            dataset = PubChemSTM_SubDatasets_Graph(dataset_root, size=10000)\n",
    "        elif args.dataset == \"PubChemSTM_Raw\":\n",
    "            dataset = PubChemSTM_Datasets_Raw_Graph(dataset_root)\n",
    "        elif args.dataset == \"PubChemSTM1K_Raw\":\n",
    "            dataset = PubChemSTM_SubDatasets_Raw_Graph(dataset_root, size=1000)\n",
    "        elif args.dataset == \"PubChemSTM10K_Raw\":\n",
    "            dataset = PubChemSTM_SubDatasets_Raw_Graph(dataset_root, size=10000)\n",
    "        dataloader_class = pyg_DataLoader\n",
    "        molecule_node_model = GNN(\n",
    "            num_layer=args.num_layer, emb_dim=args.gnn_emb_dim,\n",
    "            JK=args.JK, drop_ratio=args.dropout_ratio,\n",
    "            gnn_type=args.gnn_type)\n",
    "        molecule_model = GNN_graphpred(\n",
    "            num_layer=args.num_layer, emb_dim=args.gnn_emb_dim, JK=args.JK, graph_pooling=args.graph_pooling,\n",
    "            num_tasks=1, molecule_node_model=molecule_node_model)\n",
    "        pretrained_model_path = os.path.join(args.dataspace_path, \"pretrained_GraphMVP\", args.pretrain_gnn_mode, \"model.pth\")\n",
    "        molecule_model.from_pretrained(pretrained_model_path)\n",
    "\n",
    "        molecule_model = molecule_model.to(device)\n",
    "\n",
    "        kwargs[\"molecule_model\"] = molecule_model\n",
    "        molecule_dim = args.gnn_emb_dim\n",
    "\n",
    "    else:\n",
    "        raise Exception\n",
    "    dataloader = dataloader_class(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "\n",
    "    text2latent = nn.Linear(text_dim, args.SSL_emb_dim).to(device)\n",
    "    mol2latent = nn.Linear(molecule_dim, args.SSL_emb_dim).to(device)\n",
    "\n",
    "    if args.representation_frozen:\n",
    "        print(\"Representation is fronzen during pretraining.\")\n",
    "        freeze_network(text_model)\n",
    "        freeze_network(molecule_model)\n",
    "        model_param_group = [\n",
    "            {\"params\": text2latent.parameters(), \"lr\": args.text_lr * args.text_lr_scale},\n",
    "            {\"params\": mol2latent.parameters(), \"lr\": args.mol_lr * args.mol_lr_scale},\n",
    "        ]\n",
    "    else:\n",
    "        model_param_group = [\n",
    "            {\"params\": text_model.parameters(), \"lr\": args.text_lr},\n",
    "            {\"params\": molecule_model.parameters(), \"lr\": args.mol_lr},\n",
    "            {\"params\": text2latent.parameters(), \"lr\": args.text_lr * args.text_lr_scale},\n",
    "            {\"params\": mol2latent.parameters(), \"lr\": args.mol_lr * args.mol_lr_scale},\n",
    "        ]\n",
    "    optimizer = optim.Adam(model_param_group, weight_decay=args.decay)\n",
    "    optimal_loss = 1e10\n",
    "\n",
    "    for e in range(1, args.epochs+1):\n",
    "        print(\"Epoch {}\".format(e))\n",
    "        train(e, dataloader, **kwargs)\n",
    "    \n",
    "    save_model(save_best=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current progress in reproducing this paper has been limited given the extensive issues regarding the initial setup and preprocessing of the data. Configuring the proper environment setup and the preprocessing of the data in sum took ~2 weeks to get through. I'm currently at the point where MoleculeSTM is being pretrained for 100 epochs, as described in the original paper. Unfortunately, although the script has been running for several days, it's still only managed to get through 8/100 epochs in total. I've reached out to Shangchao Lui again for guidance and attempting to understand how they trained the model from a computation perspective, but they have yet to respond to this inquiry. \n",
    "\n",
    "To that end, I unfortunately do not have much to show in terms of results at this stage. I'm dubious that I will find a way to increase the speed of the pretraining, and at this rate there simply isn't enough time alloted for the remainder of the semester for me to complete the training and have time to submit more comprehensive results. To that point, the next section will detail all of the progress made thus far on this project, including setbacks and future plans to ensure that I have a feasible workload for the final project submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, in lieu of tangible results given the current state of the pretraining process, we include this discussion section to cover the feasibility of this project given issues we've faced and our future plans to adapt to these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the state of my initial results and the unexpected computational complexity of training, I do not believe that this paper will be reproducible within the timeline of this project. I even reached out to Shangchao Liu, the first author of the original paper, for guidance and while he indicated that he was \"pretty sure\" the results could be reproduced, he acknowledged that the setup, processing, and training would be a painful process. To that point, even after having successfully completed the initial setup, preprocessing, and beginning the pretraining process with an NVIDA RTX 4090 GPU running for the past 4 days, I'm still only on epoch 8/100; therefore, given these complexities, I do not believe that this project will be reproducible within the scope of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setbacks and Diffuclties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most major setback I've faced thus far in reproducing the results of this paper have been the significant amount of time sunk into the intial setup and preprocessing of the data. From a setup standpoint, I realized early on in my testing that many of the packages required for running this model are either deprecated or suffer from version incompatibilities. In total, I made 23 setup attempts with only the last succeeding. A significant amount of time has been invested in refactoring the source code, testing different version combinations for packages, using 3 different operating systems, and reaching out to the author for support. To finally get through the setup process, I had to take everything I had learned from my testing and apply it to a brand new computer I purchased exclusively to run Ubuntu, per the suggestion of the author. The several open issues on the GitHub repo containing the original source code, and the analysis of my peer groups trying to replicate this paper have confirmed my own experience that the setup for this project needs to be improved substantially to enable future researchers to replicate this work.\n",
    "\n",
    "Another major setback I've experienced has been with regard to computational complexity. I assumed, naively, that my RTX 4090 card that I've used for training other models without issue would be suitable for this project. In reality, even with the top-of-the-line consumer GPU at my disposal, the pretraining process is taking an exhaustive and prohibitive amount of time. Though I've left the pretraining script running for several days at this point, it has only completed 8 of the 100 epochs necessary to complete. The authors make note of checkpoints with a pretrained model that can be used, and I considered doing this to cut down on time, but if I'm only using their pretrained model that seems to defeat the point of this project. Moreover, even if I continue to allow the pretraining script to run, I've estimated that it will take ~30 days to complete at its current pace, meaning even by the end of the semester, I would have close to nothing to show for all my efforts in attempting to reproduce this paper. To that end, as I will expound on the next section, I believe it would be warranted for me to switch my project topic and idenfity one of my backup papers that will be more feasible to reproduce for he purposes of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given everything discussed in the previous sections regarding the difficulty of setup and the computational limitations that we're currently faced with, the best option seems to be requesting the ability to change our project to reproduce one of our backup papers instead. This is disappointing because on one hand, the research done by Liu et al. (2023) is truly groundbreaking and interesting from a deep learning perspective, but on the other hand, the lack of documentation for updated frameworks and the extensive need for computing has thrown a wrench into every expectation for this project. It would be in our best interest to switch papers as soon as possible and start again from scratch, as the current pace of the pretraining process does not bode well for our final project submission timeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liu, S., Nie, W., Wang, C., Lu, J., Qiao, Z., Liu, L., ... & Anandkumar, A. (2023). Multi-modal molecule structure–text model for text-based retrieval and editing. Nature Machine Intelligence, 5(12), 1447-1457."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
