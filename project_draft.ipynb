{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is a draft of the in-progress reproduction of the paper \"Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing\" originally authored by Liu et al. (2023).\n",
    "\n",
    "In this section we provide background on the purpose of this research, as well as an overview of the approach concerning structural and textual representation of molecules, as well as the criticality of pretraining for the MoleculeSTM modal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Drug discovery is a complex, time-consuming, and costly process that involves identifying molecules with therapeutic potential. To that point, recent progress in artificial intelligence (AI) offers a way to minimize these aspects and transform the process as a whole. \n",
    "\n",
    "The conventional approach for AI-facilitated drug discovery relies heavily on understanding the chemical structures of molecules, often overlooking the vast, unstructured textual knowledge available, which could offer insights into new drug design objectives, adaptability to text-based instructions, and predictions of complex biological activities. Indeed, Liu et al. (2023) breathe life into this notion with their introduction of MoleculeSTM, a multi-modal molecule structure-text modal that leverages both molecular structural information and textual descriptions through a contrastive learning strategy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural and Textual Representation of Molecules\n",
    "\n",
    "MoleculeSTM represents a multi-modal foundation model that concurrently learns from molecules' chemical structures and their associated textual descriptions. This dual-branch approach, incorporating a chemical structure branch and a textual description branch, allows for the integration of existing molecular structural models and scientific language models (Liu et al., 2023). The contrastive learning paradigm bridges these two branches, facilitating a comprehensive understanding of molecular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticality of Pretraining\n",
    "\n",
    "To enable training for the model given the two distinct branches described, the authors have synthesized this data into a structure-text dataset; in which each chemical structure is associated with a textual description. The open-ended format of the textual descriptions allow the model to achieve better zero-shot performance i.e. generalize to unseen data (Liu et al., 2023). Using the PubChemSTM dataset, the authors pretrain the model via contrastive learning. The general framework is to map representations from the structural and textual branches to a shared molecular model by reducing the distance between pairs of the same molecule and decreasing that between pairs for different molecules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of Reproducability\n",
    "\n",
    "### A Note On This Notebook\n",
    "\n",
    "Given the complexity of this project, it is unfortunately a very prohibitive process to migrate the code, as well as the environmental dependencies, into a Google Colab notebook. To the extent that this draft is intended to reflect progress made on this project, we will save time attempting to make all of this code fit into that format in favor on making real progress towards the end goal of reproducing results. \n",
    "\n",
    "### Hypotheses\n",
    "\n",
    "Here we list out the hypotheses from the original paper that we will be testing for our reproduction.\n",
    "\n",
    "#### Zero-shot structure-text retrieval\n",
    "\n",
    "MoleculeSTM can accurately link molecular structures to their textual descriptions without being directly trained on these specific pairs, showcasing its ability to generalize from its pretraining.\n",
    "\n",
    "#### Zero-shot text-based molecule editing\n",
    "\n",
    "MoleculeSTM can edit and generate molecular structures to meet new specifications provided via textual descriptions, demonstrating its understanding of complex chemical properties and functionalities from text alone.\n",
    "\n",
    "#### Molecular property prediction\n",
    "\n",
    "MoleculeSTM can predict molecular properties accurately, leveraging its pretraining on structure-text relationships, suggesting that it captures meaningful chemical information that is relevant for property prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the massive size of the data, as well as the need to keep the various datasets aligned with each other for the purposes of pretraining, we are unable to push the datasets with the limitatioms of pushes to a GitHub repository, nor can we simply upload a sample of the data. However, we will provide the link to download the data from HuggingFace [here](https://huggingface.co/datasets/chao1224/MoleculeSTM), similarly to how we accessed the data for ourselves. In addition, we provide this script adapted from the original code to download all the required training and downstream datasets that will be used for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "api = HfApi()\n",
    "snapshot_download(repo_id=\"chao1224/MoleculeSTM\", repo_type=\"dataset\", local_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to this, there is a required step to perform preprocessing on the PubChemSTM dataset; this is due to the fact that the PubChemSTM dataset only has the license to share the structural representations of molecules, but not the textual represetntation. So, in order to complete the dataset required for pretraining the model, you must follow the instructions below adapted from the advice of Shangchao Liu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Description Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "def clean_up_description(description):\n",
    "    description = description + \" \"\n",
    "\n",
    "    ##### extra adj Pure #####\n",
    "    if description.startswith(\"Pure \"):\n",
    "        description = description.replace(\"Pure \", \"\")\n",
    "    ##### fix typo #####\n",
    "    if description.startswith(\"Mercurycombines\"):\n",
    "        description = description.replace(\"Mercurycombines\", \"Mercury combines\")\n",
    "    \n",
    "    name_special_case_list = [\n",
    "        '17-Hydroxy-6-methylpregna-3,6-diene-3,20-dione. ',\n",
    "        '5-Thymidylic acid. ',\n",
    "        \"5'-S-(3-Amino-3-carboxypropyl)-5'-thioadenosine. \",\n",
    "        \"Guanosine 5'-(trihydrogen diphosphate), monoanhydride with phosphorothioic acid. \",\n",
    "        \"5'-Uridylic acid. \",\n",
    "        \"5'-Adenylic acid, \",\n",
    "        \"Uridine 5'-(tetrahydrogen triphosphate). \",\n",
    "        \"Inosine 5'-Monophosphate. \",\n",
    "        \"Pivaloyloxymethyl butyrate (AN-9), \",\n",
    "        \"4-Amino-5-cyano-7-(D-ribofuranosyl)-7H- pyrrolo(2,3-d)pyrimidine. \",\n",
    "        \"Cardamonin (also known as Dihydroxymethoxychalcone), \",\n",
    "    ]\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"17-Hydroxy-6-methylpregna-3,6-diene-3,20-dione. \", \"17-Hydroxy-6-methylpregna-3,6-diene-3,20-dione is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"5-Thymidylic acid. \", \"5-Thymidylic acid. is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"5'-S-(3-Amino-3-carboxypropyl)-5'-thioadenosine. \", \"5'-S-(3-Amino-3-carboxypropyl)-5'-thioadenosine. is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Guanosine 5'-(trihydrogen diphosphate), monoanhydride with phosphorothioic acid. \", \"Guanosine 5'-(trihydrogen diphosphate), monoanhydride with phosphorothioic acid is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"5'-Uridylic acid. \", \"5'-Uridylic acid is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"5'-Adenylic acid, \", \"5'-Adenylic acid is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Uridine 5'-(tetrahydrogen triphosphate). \", \"Uridine 5'-(tetrahydrogen triphosphate). is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Inosine 5'-Monophosphate. \", \"Inosine 5'-Monophosphate. is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Pivaloyloxymethyl butyrate (AN-9), \", \"Pivaloyloxymethyl butyrate (AN-9) is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"4-Amino-5-cyano-7-(D-ribofuranosyl)-7H- pyrrolo(2,3-d)pyrimidine. \", \"4-Amino-5-cyano-7-(D-ribofuranosyl)-7H- pyrrolo(2,3-d)pyrimidine is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Cardamonin (also known as Dihydroxymethoxychalcone), \", \"Cardamonin (also known as Dihydroxymethoxychalcone) is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Lithium has been used to treat \", \"Lithium is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"4,4'-Methylenebis \", \"4,4'-Methylenebis is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"2,3,7,8-Tetrachlorodibenzo-p-dioxin\", \"2,3,7,8-Tetrachlorodibenzo-p-dioxin is \")\n",
    "\n",
    "    ##### a special case #####\n",
    "    description = description.replace(\"Exposure to 2,4,5-trichlorophenol \", \"2,4,5-Trichlorophenol exposure \")\n",
    "\n",
    "    index = 0\n",
    "    L = len(description)\n",
    "    if description.startswith('C.I. '):\n",
    "        start_index = len('C.I. ')\n",
    "    elif description.startswith('Nectriapyrone. D '):\n",
    "        start_index = len('Nectriapyrone. D ')\n",
    "    elif description.startswith('Salmonella enterica sv. Minnesota LPS core oligosaccharide'):\n",
    "        start_index = len('Salmonella enterica sv. Minnesota LPS core oligosaccharide')\n",
    "    else:\n",
    "        start_index = 0\n",
    "    for index in range(start_index, L - 1):\n",
    "        if index < L-2:\n",
    "            if description[index] == '.' and description[index+1] == ' ' and 'A' <= description[index+2] <= 'Z':\n",
    "                break\n",
    "        elif index == L - 2:\n",
    "            break\n",
    "    \n",
    "    first_sentence = description[:index+1]\n",
    "    return first_sentence\n",
    "\n",
    "\n",
    "def extract_name(name_raw, description):\n",
    "    first_sentence = clean_up_description(description)\n",
    "\n",
    "    splitter = '  --  --  '\n",
    "    if ' are ' in first_sentence or ' were ' in first_sentence:\n",
    "        replaced_words = 'These molecules'\n",
    "    else:\n",
    "        replaced_words = 'This molecule'\n",
    "\n",
    "    first_sentence = first_sentence.replace(' is ', splitter)\n",
    "    first_sentence = first_sentence.replace(' are ', splitter)\n",
    "    first_sentence = first_sentence.replace(' was ', splitter)\n",
    "    first_sentence = first_sentence.replace(' were ', splitter)\n",
    "    first_sentence = first_sentence.replace(' appears ', splitter)\n",
    "    first_sentence = first_sentence.replace(' occurs ', splitter)\n",
    "    first_sentence = first_sentence.replace(' stands for ', splitter)\n",
    "    first_sentence = first_sentence.replace(' belongs to ', splitter)\n",
    "    first_sentence = first_sentence.replace(' exists ', splitter) # only for CID=11443\n",
    "    first_sentence = first_sentence.replace(' has been used in trials ', splitter)\n",
    "    first_sentence = first_sentence.replace(' has been investigated ', splitter)\n",
    "    first_sentence = first_sentence.replace(' has many uses ', splitter)\n",
    "    \n",
    "    if splitter in first_sentence:\n",
    "        extracted_name = first_sentence.split(splitter, 1)[0]\n",
    "    elif first_sentence.startswith(name_raw):\n",
    "        extracted_name = name_raw\n",
    "    elif name_raw in first_sentence:\n",
    "        extracted_name = name_raw\n",
    "        extracted_name = None\n",
    "        print(\"=====\", name_raw)\n",
    "        print(\"first sentence: \", first_sentence)\n",
    "        # print()\n",
    "    else:\n",
    "        extracted_name = None\n",
    "\n",
    "    if extracted_name is not None:\n",
    "        extracted_description = description.replace(extracted_name, replaced_words)\n",
    "    else:\n",
    "        extracted_description = description\n",
    "\n",
    "    return extracted_name, extracted_description, first_sentence\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_page_num = 290\n",
    "    # Please put your own dataset path here\n",
    "    datasets_home_folder = \"../../../Datasets\"\n",
    "\n",
    "    PubChemSTM_datasets_description_home_folder = \"{}/step_01_PubChemSTM_description\".format(datasets_home_folder)\n",
    "    valid_CID_list = set()\n",
    "    CID2name_raw, CID2name_extracted = defaultdict(list), defaultdict(list)\n",
    "    CID2text_raw, CID2text_extracted = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for page_index in tqdm(range(total_page_num)):\n",
    "        page_num = page_index + 1\n",
    "        compound_description_file_name = \"Compound_description_{}.txt\".format(page_num)\n",
    "        f_out = open(\"{}/{}\".format(PubChemSTM_datasets_description_home_folder, compound_description_file_name), \"w\")\n",
    "        \n",
    "        description_url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/annotations/heading/json?heading_type=Compound&heading=Record+Description&page={}\".format(page_num)\n",
    "        description_data = requests.get(description_url).json()\n",
    "\n",
    "        description_data = description_data[\"Annotations\"]\n",
    "        assert description_data[\"Page\"] == page_num\n",
    "        assert description_data[\"TotalPages\"] == total_page_num\n",
    "        \n",
    "        record_list = description_data[\"Annotation\"]\n",
    "        \n",
    "        for record in record_list:\n",
    "            try:\n",
    "                CID = record[\"LinkedRecords\"][\"CID\"][0]\n",
    "                if \"Name\" in record:\n",
    "                    name_raw = record[\"Name\"]\n",
    "                    CID2name_raw[CID].append(name_raw)\n",
    "                else:\n",
    "                    name_raw = None\n",
    "\n",
    "                data_list = record[\"Data\"]\n",
    "                for data in data_list:\n",
    "                    description = data[\"Value\"][\"StringWithMarkup\"][0][\"String\"].strip()\n",
    "                    \n",
    "                    extracted_name, extracted_description, first_sentence = extract_name(name_raw, description)\n",
    "                    if extracted_name is not None:\n",
    "                        CID2name_extracted[CID].append(extracted_name)\n",
    "\n",
    "                    CID_special_case_list = [45266824, 11683, 3759, 9700, 439155, 135398675, 135563708, 6030, 10238, 6133, 135398640, 77918, 60748, 11824, 641785, 11125, 7543, 15625, 7271]\n",
    "\n",
    "                    ##### only for debugging #####\n",
    "                    if CID in CID_special_case_list:\n",
    "                        print(\"page: {}\\tCID: {}\".format(page_index, CID))                \n",
    "                        if \"Name\" in record:\n",
    "                            print('yes-name')\n",
    "                            name = record[\"Name\"]\n",
    "                            print('name:', name)\n",
    "                        else:\n",
    "                            print('no-name')\n",
    "                        print('extracted name:', extracted_name)\n",
    "                        print(\"first_sentence:\", first_sentence)\n",
    "                        print(\"extracted_description:\", extracted_description)\n",
    "                        print(\"description:\", description)\n",
    "                        print() \n",
    "                        \n",
    "                    CID2text_raw[CID].append(description)\n",
    "                    CID2text_extracted[CID].append(extracted_description)\n",
    "\n",
    "                    valid_CID_list.add(CID)\n",
    "                    f_out.write(\"{}\\n\".format(CID))\n",
    "                    f_out.write(\"{}\\n\\n\".format(extracted_description))\n",
    "            except:\n",
    "                # print(\"===\\n\", record)\n",
    "                # print(\"missing page: {}\\tSourceName: {}\\tSourceID: {}\".format(page_index, record['SourceName'], record['SourceID']))\n",
    "                continue\n",
    "            \n",
    "    valid_CID_list = list(set(valid_CID_list))\n",
    "    valid_CID_list = sorted(valid_CID_list)\n",
    "    # print(\"valid CID list: {}\".format(valid_CID_list))\n",
    "    print(\"Total CID (with raw name) {}\".format(len(CID2name_raw)))\n",
    "    print(\"Total CID (with extracted name) {}\".format(len(CID2name_extracted)))\n",
    "    print(\"Total CID {}\".format(len(valid_CID_list)))\n",
    "    \n",
    "    with open(\"{}/PubChemSTM_data/raw/CID2name_raw.json\".format(datasets_home_folder), \"w\") as f:\n",
    "        json.dump(CID2name_raw, f)\n",
    "    \n",
    "    with open(\"{}/PubChemSTM_data/raw/CID2name.json\".format(datasets_home_folder), \"w\") as f:\n",
    "        json.dump(CID2name_extracted, f)\n",
    "\n",
    "    with open(\"{}/PubChemSTM_data/raw/CID2text_raw.json\".format(datasets_home_folder), \"w\") as f:\n",
    "        json.dump(CID2text_raw, f)\n",
    "\n",
    "    with open(\"{}/PubChemSTM_data/raw/CID2text.json\".format(datasets_home_folder), \"w\") as f:\n",
    "        json.dump(CID2text_extracted, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download SDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from PubChem_utils import download_and_extract_compound_file\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--block_id\", type=int, default=0)\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    datasets_home_folder = \"../../../Datasets\"\n",
    "\n",
    "    PubChemSTM_datasets_home_folder = \"{}/step_02_PubChemSTM_SDF\".format(datasets_home_folder)\n",
    "    block_id = args.block_id\n",
    "    block_size = 500000\n",
    "    start_id = block_id * block_size + 1\n",
    "    end_id = (block_id + 1) * block_size\n",
    "\n",
    "    compound_file_name = \"Compound_{:09d}_{:09d}.sdf.gz\".format(start_id, end_id)\n",
    "    download_and_extract_compound_file(PubChemSTM_datasets_home_folder, compound_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from collections.abc import Sequence\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, batch_norm=False, activation=\"relu\", dropout=0):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        if not isinstance(hidden_dims, Sequence):\n",
    "            hidden_dims = [hidden_dims]\n",
    "        self.dims = [input_dim] + hidden_dims\n",
    "\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(self.dims) - 1):\n",
    "            self.layers.append(nn.Linear(self.dims[i], self.dims[i + 1]))\n",
    "        if batch_norm:\n",
    "            self.batch_norms = nn.ModuleList()\n",
    "            for i in range(len(self.dims) - 2):\n",
    "                self.batch_norms.append(nn.BatchNorm1d(self.dims[i + 1]))\n",
    "        else:\n",
    "            self.batch_norms = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        layer_input = input\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            hidden = layer(layer_input)\n",
    "            if i < len(self.layers) - 1:\n",
    "                if self.batch_norms:\n",
    "                    x = hidden.flatten(0, -2)\n",
    "                    hidden = self.batch_norms[i](x).view_as(hidden)\n",
    "                hidden = self.activation(hidden)\n",
    "                if self.dropout:\n",
    "                    hidden = self.dropout(hidden)\n",
    "            if hidden.shape == layer_input.shape:\n",
    "                hidden = hidden + layer_input\n",
    "            layer_input = hidden\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the state of my initial results and the unexpected computational complexity of training, I do not believe that this paper will be reproducible within the timeline of this project. I even reached out to Shangchao Liu, the first author of the original paper, for guidance and while he indicated that he was \"pretty sure\" the results could be reproduced, he acknowledged that the setup, processing, and training would be a painful process. To that point, even after having successfully completed the initial setup, preprocessing, and beginning the pretraining process with an NVIDA RTX 4090 GPU running for the past 4 days, I'm still only on epoch 8/100; therefore, given these complexities, I do not believe that this project will be reproducible within the scope of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setbacks and Diffuclties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most major setback I've faced thus far in reproducing the results of this paper have been the significant amount of time sunk into the intial setup and preprocessing of the data. From a setup standpoint, I realized early on in my testing that many of the packages required for running this model are either deprecated or suffer from version incompatibilities. In total, I made 23 setup attempts with only the last succeeding. A significant amount of time has been invested in refactoring the source code, testing different version combinations for packages, using 3 different operating systems, and reaching out to the author for support. To finally get through the setup process, I had to take everything I had learned from my testing and apply it to a brand new computer I purchased exclusively to run Ubuntu, per the suggestion of the author. The several open issues on the GitHub repo containing the original source code, and the analysis of my peer groups trying to replicate this paper have confirmed my own experience that the setup for this project needs to be improved substantially to enable future researchers to replicate this work.\n",
    "\n",
    "Another major setback I've experienced has been with regard to computational complexity. I assumed, naively, that my RTX 4090 card that I've used for training other models without issue would be suitable for this project. In reality, even with the top-of-the-line consumer GPU at my disposal, the pretraining process is taking an exhaustive and prohibitive amount of time. Though I've left the pretraining script running for several days at this point, it has only completed 8 of the 100 epochs necessary to complete. The authors make note of checkpoints with a pretrained model that can be used, and I considered doing this to cut down on time, but if I'm only using their pretrained model that seems to defeat the point of this project. Moreover, even if I continue to allow the pretraining script to run, I've estimated that it will take ~30 days to complete at its current pace, meaning even by the end of the semester, I would have close to nothing to show for all my efforts in attempting to reproduce this paper. To that end, as I will expound on the next section, I believe it would be warranted for me to switch my project topic and idenfity one of my backup papers that will be more feasible to reproduce for he purposes of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given everything discussed in the previous sections regarding the difficulty of setup and the computational limitations that we're currently faced with, the best option seems to be requesting the ability to change our project to reproduce one of our backup papers instead. This is disappointing because on one hand, the research done by Liu et al. (2023) is truly groundbreaking and interesting from a deep learning perspective, but on the other hand, the lack of documentation for updated frameworks and the extensive need for computing has thrown a wrench into every expectation for this project. It would be in our best interest to switch papers as soon as possible and start again from scratch, as the current pace of the pretraining process does not bode well for our final project submission timeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liu, S., Nie, W., Wang, C., Lu, J., Qiao, Z., Liu, L., ... & Anandkumar, A. (2023). Multi-modal molecule structureâ€“text model for text-based retrieval and editing. Nature Machine Intelligence, 5(12), 1447-1457."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
